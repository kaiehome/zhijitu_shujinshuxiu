# auto_tune_ai.py
# AIå¢å¼ºç‰ˆè‡ªåŠ¨åŒ–å¾®è°ƒä¸è¯¯å·®åˆ†æè„šæœ¬

import cv2
import numpy as np
import os
import json
from datetime import datetime
from analysis_tools import AnalysisTools
from image_processor import SichuanBrocadeProcessor
from ai_segmentation import AISegmenter
from feature_detector import FeatureDetector

# å¯é€‰ï¼šé£æ ¼è¿ç§»/è‰²å½©æ˜ å°„æ–¹æ³•
try:
    from skimage.exposure import match_histograms
    STYLE_TRANSFER_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: scikit-image ä¸å¯ç”¨ï¼Œå°†è·³è¿‡é£æ ¼è¿ç§»åŠŸèƒ½")
    STYLE_TRANSFER_AVAILABLE = False

def auto_tune_ai(
    orig_path: str,
    pred_path: str,
    gt_path: str,
    out_dir: str = "auto_tune_ai_results",
    n_colors_list = [16, 18, 20],
    smooth_kernel_list = [3, 5, 7],
    style_transfer: bool = True,
    ai_segment_models = ['grabcut', 'watershed', 'slic', 'contour'],
    feature_models = ['opencv', 'contour', 'blob'],
    use_feature_guided_segmentation: bool = True
):
    """
    AIå¢å¼ºç‰ˆè‡ªåŠ¨åŒ–å¾®è°ƒ
    
    Args:
        orig_path: åŸå›¾è·¯å¾„
        pred_path: ç¨‹åºç”Ÿæˆè¯†åˆ«å›¾è·¯å¾„
        gt_path: ç›®æ ‡è¯†åˆ«å›¾è·¯å¾„
        out_dir: è¾“å‡ºç›®å½•
        n_colors_list: è‰²å½©æ•°é‡åˆ—è¡¨
        smooth_kernel_list: å¹³æ»‘æ ¸å¤§å°åˆ—è¡¨
        style_transfer: æ˜¯å¦å¯ç”¨é£æ ¼è¿ç§»
        ai_segment_models: AIåˆ†å‰²æ¨¡å‹åˆ—è¡¨
        feature_models: ç‰¹å¾æ£€æµ‹æ¨¡å‹åˆ—è¡¨
        use_feature_guided_segmentation: æ˜¯å¦ä½¿ç”¨ç‰¹å¾å¼•å¯¼åˆ†å‰²
    """
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(out_dir, exist_ok=True)
    
    # åŠ è½½å›¾åƒ
    orig = cv2.imread(orig_path)
    pred = cv2.imread(pred_path)
    gt = cv2.imread(gt_path)
    
    # å¥å£®æ€§æ£€æŸ¥
    if orig is None:
        raise FileNotFoundError(f'åŸå›¾æœªæ‰¾åˆ°: {orig_path}')
    if pred is None:
        raise FileNotFoundError(f'ç¨‹åºç”Ÿæˆè¯†åˆ«å›¾æœªæ‰¾åˆ°: {pred_path}')
    if gt is None:
        raise FileNotFoundError(f'ç›®æ ‡è¯†åˆ«å›¾æœªæ‰¾åˆ°: {gt_path}')
    
    print(f"âœ… å›¾åƒåŠ è½½æˆåŠŸ:")
    print(f"   åŸå›¾: {orig.shape}")
    print(f"   ç¨‹åºç”Ÿæˆå›¾: {pred.shape}")
    print(f"   ç›®æ ‡å›¾: {gt.shape}")
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = SichuanBrocadeProcessor()
    
    # åˆå§‹åŒ–AIåˆ†å‰²å™¨
    segmenters = {}
    for model_name in ai_segment_models:
        try:
            segmenters[model_name] = AISegmenter(model_name=model_name)
            print(f"âœ… AIåˆ†å‰²å™¨ {model_name} åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ AIåˆ†å‰²å™¨ {model_name} åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # åˆå§‹åŒ–ç‰¹å¾æ£€æµ‹å™¨
    detectors = {}
    for model_name in feature_models:
        try:
            detectors[model_name] = FeatureDetector(model_name=model_name)
            print(f"âœ… ç‰¹å¾æ£€æµ‹å™¨ {model_name} åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾æ£€æµ‹å™¨ {model_name} åˆå§‹åŒ–å¤±è´¥: {e}")
    
    results = []
    total_combinations = len(n_colors_list) * len(smooth_kernel_list) * len(segmenters) * len(detectors)
    current_combination = 0
    
    print(f"\nğŸš€ å¼€å§‹AIå¢å¼ºå¾®è°ƒï¼Œå…± {total_combinations} ç§å‚æ•°ç»„åˆ...")
    
    for n_colors in n_colors_list:
        for kernel in smooth_kernel_list:
            for seg_model_name, segmenter in segmenters.items():
                for feat_model_name, detector in detectors.items():
                    current_combination += 1
                    print(f"\nğŸ“Š è¿›åº¦: {current_combination}/{total_combinations}")
                    print(f"   å‚æ•°: è‰²å½©={n_colors}, æ ¸={kernel}, åˆ†å‰²={seg_model_name}, ç‰¹å¾={feat_model_name}")
                    
                    try:
                        # 1. AIåˆ†å‰²
                        seg_params = {
                            'n_segments': 100 if seg_model_name == 'slic' else None,
                            'compactness': 10 if seg_model_name == 'slic' else None
                        }
                        seg_mask = segmenter.segment(orig, seg_params)
                        
                        # 2. ç‰¹å¾ç‚¹æ£€æµ‹
                        feat_params = {
                            'face_scale_factor': 1.1,
                            'face_min_neighbors': 5,
                            'eye_scale_factor': 1.1,
                            'eye_min_neighbors': 3
                        }
                        keypoints = detector.detect(orig, feat_params)
                        
                        # 3. ç‰¹å¾å¼•å¯¼åˆ†å‰²ï¼ˆå¯é€‰ï¼‰
                        if use_feature_guided_segmentation and keypoints:
                            seg_mask = _apply_feature_guidance(seg_mask, keypoints, orig.shape)
                        
                        # 4. åº”ç”¨åˆ†å‰²æ©ç 
                        pred_proc = pred.copy()
                        if seg_mask is not None and seg_mask.sum() > 0:
                            # å°†æ©ç åº”ç”¨åˆ°å¤„ç†å›¾åƒ
                            pred_proc = cv2.bitwise_and(pred_proc, pred_proc, mask=seg_mask)
                            # å°†èƒŒæ™¯è®¾ä¸ºç™½è‰²
                            pred_proc[seg_mask == 0] = [255, 255, 255]
                        
                        # 5. é£æ ¼è¿ç§»/è‰²å½©æ˜ å°„
                        if style_transfer and STYLE_TRANSFER_AVAILABLE:
                            pred_proc = match_histograms(pred_proc, gt, channel_axis=-1)
                        
                        # 6. è‰²å½©èšç±»
                        quant_img, color_table = processor.color_quantize(pred_proc, n_colors=n_colors)
                        
                        # 7. è¾¹ç•Œå¹³æ»‘
                        mask = cv2.cvtColor(quant_img, cv2.COLOR_BGR2GRAY)
                        mask = processor.smooth_boundary(mask, kernel_size=kernel)
                        quant_img[mask == 0] = 255
                        
                        # 8. è¯¯å·®åˆ†æ
                        report, diff_vis = AnalysisTools.compare(gt, quant_img)
                        
                        # 9. ä¿å­˜ç»“æœ
                        tag = f"n{n_colors}_k{kernel}_seg{seg_model_name}_feat{feat_model_name}_style{int(style_transfer)}"
                        out_img = os.path.join(out_dir, f"result_{tag}.png")
                        out_diff = os.path.join(out_dir, f"diff_{tag}.png")
                        out_csv = os.path.join(out_dir, f"color_table_{tag}.csv")
                        out_mask = os.path.join(out_dir, f"mask_{tag}.png")
                        
                        cv2.imwrite(out_img, quant_img)
                        cv2.imwrite(out_diff, diff_vis)
                        cv2.imwrite(out_mask, seg_mask)
                        processor.export_color_table(color_table, out_csv)
                        
                        # 10. è®°å½•ç»“æœ
                        result = {
                            'params': {
                                'n_colors': n_colors,
                                'kernel': kernel,
                                'segmentation_model': seg_model_name,
                                'feature_model': feat_model_name,
                                'style_transfer': style_transfer,
                                'feature_guided': use_feature_guided_segmentation
                            },
                            'report': report,
                            'keypoints_count': len(keypoints),
                            'result_img': out_img,
                            'diff_img': out_diff,
                            'color_table': out_csv,
                            'mask_img': out_mask,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        results.append(result)
                        print(f"   âœ… å®Œæˆï¼Œåƒç´ å·®å¼‚ç‡: {report['åƒç´ å·®å¼‚ç‡']:.4f}")
                        
                    except Exception as e:
                        print(f"   âŒ å¤±è´¥: {e}")
                        continue
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_path = os.path.join(out_dir, "ai_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆæœ€ä½³ç»“æœåˆ†æ
    _generate_best_results_analysis(results, out_dir)
    
    print(f"\nğŸ‰ AIå¢å¼ºå¾®è°ƒå®Œæˆï¼")
    print(f"   ç»“æœä¿å­˜åœ¨: {out_dir}")
    print(f"   æˆåŠŸç»„åˆæ•°: {len(results)}/{total_combinations}")
    print(f"   æ±‡æ€»æŠ¥å‘Š: {summary_path}")

def _apply_feature_guidance(mask: np.ndarray, keypoints: dict, image_shape: tuple) -> np.ndarray:
    """åº”ç”¨ç‰¹å¾ç‚¹å¼•å¯¼ä¼˜åŒ–åˆ†å‰²æ©ç """
    enhanced_mask = mask.copy()
    
    # æ ¹æ®æ£€æµ‹åˆ°çš„ç‰¹å¾ç‚¹ä¼˜åŒ–æ©ç 
    for feature_name, feature_data in keypoints.items():
        if 'center' in feature_data:
            center = feature_data['center']
            if isinstance(center, tuple) and len(center) == 2:
                x, y = center
                # åœ¨ç‰¹å¾ç‚¹å‘¨å›´åˆ›å»ºåœ†å½¢åŒºåŸŸ
                cv2.circle(enhanced_mask, (int(x), int(y)), 20, 255, -1)
        
        if 'bbox' in feature_data:
            bbox = feature_data['bbox']
            if isinstance(bbox, tuple) and len(bbox) == 4:
                x, y, w, h = bbox
                # åœ¨è¾¹ç•Œæ¡†åŒºåŸŸå¡«å……
                cv2.rectangle(enhanced_mask, (int(x), int(y)), (int(x+w), int(y+h)), 255, -1)
    
    return enhanced_mask

def _generate_best_results_analysis(results: list, out_dir: str):
    """ç”Ÿæˆæœ€ä½³ç»“æœåˆ†æ"""
    if not results:
        return
    
    # æŒ‰åƒç´ å·®å¼‚ç‡æ’åº
    sorted_results = sorted(results, key=lambda x: x['report']['åƒç´ å·®å¼‚ç‡'])
    
    # ç”Ÿæˆæœ€ä½³ç»“æœæŠ¥å‘Š
    best_results = {
        'best_overall': sorted_results[0],
        'best_by_segmentation': {},
        'best_by_feature': {},
        'statistics': {
            'total_combinations': len(results),
            'avg_pixel_diff': np.mean([r['report']['åƒç´ å·®å¼‚ç‡'] for r in results]),
            'min_pixel_diff': min([r['report']['åƒç´ å·®å¼‚ç‡'] for r in results]),
            'max_pixel_diff': max([r['report']['åƒç´ å·®å¼‚ç‡'] for r in results])
        }
    }
    
    # æŒ‰åˆ†å‰²æ¨¡å‹åˆ†ç»„æœ€ä½³ç»“æœ
    seg_models = set(r['params']['segmentation_model'] for r in results)
    for seg_model in seg_models:
        seg_results = [r for r in results if r['params']['segmentation_model'] == seg_model]
        best_seg = min(seg_results, key=lambda x: x['report']['åƒç´ å·®å¼‚ç‡'])
        best_results['best_by_segmentation'][seg_model] = best_seg
    
    # æŒ‰ç‰¹å¾æ¨¡å‹åˆ†ç»„æœ€ä½³ç»“æœ
    feat_models = set(r['params']['feature_model'] for r in results)
    for feat_model in feat_models:
        feat_results = [r for r in results if r['params']['feature_model'] == feat_model]
        best_feat = min(feat_results, key=lambda x: x['report']['åƒç´ å·®å¼‚ç‡'])
        best_results['best_by_feature'][feat_model] = best_feat
    
    # ä¿å­˜æœ€ä½³ç»“æœåˆ†æ
    best_analysis_path = os.path.join(out_dir, "best_results_analysis.json")
    with open(best_analysis_path, 'w', encoding='utf-8') as f:
        json.dump(best_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ† æœ€ä½³ç»“æœåˆ†æ:")
    print(f"   æœ€ä½³æ•´ä½“: {best_results['best_overall']['params']}")
    print(f"   åƒç´ å·®å¼‚ç‡: {best_results['best_overall']['report']['åƒç´ å·®å¼‚ç‡']:.4f}")
    print(f"   åˆ†ææŠ¥å‘Š: {best_analysis_path}")

if __name__ == "__main__":
    # è¿è¡ŒAIå¢å¼ºå¾®è°ƒ
    auto_tune_ai(
        orig_path="orig.png",
        pred_path="pred.png", 
        gt_path="gt.png",
        out_dir="auto_tune_ai_results",
        n_colors_list=[16, 18, 20],
        smooth_kernel_list=[3, 5, 7],
        style_transfer=True,
        ai_segment_models=['grabcut', 'watershed', 'slic', 'contour'],
        feature_models=['opencv', 'contour', 'blob'],
        use_feature_guided_segmentation=True
    ) 